input {
  beats {
    port => {{ LOGSTASH_PORT }}
    type => beats
  }
}

filter {
  multiline {
    pattern => "^\s"
    what => "previous"
  }
  if ("{{ nginx_log_dir }}/kibana.access.log" in [source]) {
    grok {
      match => { "message" => "%{IP:ip_addr} \- \- \[%{MONTHDAY}\/%{MONTH}\/%{YEAR}\:%{HOUR}:?%{MINUTE}\:%{SECOND} %{ISO8601_TIMEZONE}\] \"%{WORD:method} %{GREEDYDATA:syslog_message}" }
    }
  } else if ("{{ COMMON_LOG_DIR }}/tracking/tracking.log" in [source]) {
    #{"username": "", "event_type": "/", "ip": "192.168.33.1", "agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/601.6.17 (KHTML, like Gecko) Version/9.1.1 Safari/601.6.17", "host": "precise64", "referer": "", "accept_language": "en-us", "event": "{\"POST\": {}, \"GET\": {}}", "event_source": "server", "context": {"user_id": "", "org_id": "", "course_id": "", "path": "/"}, "time": "2016-07-15T21:08:52.465455+00:00", "page": null}

#mongo
#2016-07-15T22:47:36.436+0000 [clientcursormon]  mapped:240

#rabbitmq
#date='Fri Jul 15 22:46:02 UTC 2016' vhost='/' queue='edx.cms.core.default' length=3

#logstash
#{:timestamp=>"2016-07-15T22:56:34.845000+0000", :message=>"Pipeline main has been shutdown"}
#{:timestamp=>"2016-07-15T22:56:46.967000+0000", :message=>"Defaulting pipeline worker threads to 1 because there are some filters that might not work with multiple worker threads", :count_was=>2, :filters=>["multiline"], :level=>:warn}

#edx.log (cms, lms, notifier)
#Jul 15 22:45:50 precise64 [service_variant=cms][elasticsearch][env:sandbox] INFO [precise64  27884] [base.py:49] - HEAD http://localhost:9200/courseware_index [status:200 request:0.005s]
#Jul 15 22:45:53 precise64 [service_variant=cms][xmodule.modulestore.django][env:sandbox] INFO [precise64  27884] [django.py:112] - Sent course_published signal to <function _listen_for_course_publish at 0x4a52d70> with kwargs {'course_key': CourseLocator(u'edX', u'DemoX', u'Demo_Course', None, None)}. Response was: None

#devpi
#2016-07-15 16:16:31,287 INFO  NOCTX serverdir: /edx/var/devpi/data

#supervisor
#2016-03-07 22:54:57,636 INFO supervisord started with pid 25561
2016-03-07 22:54:57,629 WARN No file matches via include "/edx/app/supervisor/conf.d/*.conf"

  }else { #matches all rsyslog messages
    grok {
       match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp}%{SPACE}%{WORD:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
    }
  }
  syslog_pri { }
  date {
    match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss", "DD MMM YYYY:HH:mm:ss ZZZ"]
  }
  # Try and parse the tracking log json
  # 142 is syslog facility 17 (local1) and Informational.
  # This is used to reduce the number of errors in json parsing as
  # tracking uses that facility and priority by default.
  if "142" in [syslog_pri] {
    json {
      source => "syslog_message"
      target => "tracking"
    }
  }
  if !("_grokparsefailure" in [tags]) {
    mutate {
      replace => [ "@source_host", "%{syslog_hostname}" ]
      replace => [ "@message", "%{syslog_message}" ]
    }
#    mutate {
#      remove_field => [ "syslog_hostname", "syslog_message", "syslog_timestamp" ]
#    }
  }
}

output {
  # Example just to output to elasticsearch
  elasticsearch { }
  # And gzip for each host and program
  file {
       path => '{{ logstash_data_dir }}/%{beat.hostname}/all.%{+yyyyMMdd}.gz'
       gzip => true
  }
  # Should add option for S3 as well.
}
