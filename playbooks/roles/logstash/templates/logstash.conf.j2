input {
  beats {
    port => {{ LOGSTASH_PORT }}
    type => beats
  }
}

filter {
  multiline {
    pattern => "^\s"
    what => "previous"
  }
  if ("{{ nginx_log_dir }}/kibana.access.log" in [source]) {
    grok {
      match => { "message" => "%{IP:ip_addr} \- \- \[%{MONTHDAY}\/%{MONTH}\/%{YEAR}\:%{HOUR}:?%{MINUTE}\:%{SECOND} %{ISO8601_TIMEZONE}\] \"%{WORD:method} %{GREEDYDATA:syslog_message}" }
    }
  } else { #matches all rsyslog messages
    grok {
       match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp}%{SPACE}%{WORD:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
    }
  }
  syslog_pri { }
  date {
    match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss", "DD MMM YYYY:HH:mm:ss ZZZ"]
  }
  # Try and parse the tracking log json
  # 142 is syslog facility 17 (local1) and Informational.
  # This is used to reduce the number of errors in json parsing as
  # tracking uses that facility and priority by default.
  if "142" in [syslog_pri] {
    json {
      source => "syslog_message"
      target => "tracking"
    }
  }
  if !("_grokparsefailure" in [tags]) {
    mutate {
      replace => [ "@source_host", "%{syslog_hostname}" ]
      replace => [ "@message", "%{syslog_message}" ]
    }
    mutate {
      remove_field => [ "syslog_hostname", "syslog_message", "syslog_timestamp" ]
    }
  }
}

output {
  # Example just to output to elasticsearch
  elasticsearch { }
  # And gzip for each host and program
  file {
       path => '{{ logstash_data_dir }}/%{beat.hostname}/all.%{+yyyyMMdd}.gz'
       gzip => true
  }
  # Should add option for S3 as well.
}
