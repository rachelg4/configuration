input {
  beats {
    port => {{ LOGSTASH_PORT }}
    type => beats
  }
}

filter {
  multiline {
    pattern => "^\s"
    what => "previous"
  }
  if ("multiline" in [tags]) {
    grok {
      match => { "message" => "%{GREEDYDATA:syslog_message}" }
    }
  } else if ("{{ nginx_log_dir }}/kibana.access.log" in [source]) {
    grok {
      match => { "message" => "%{IP:ip_addr} \- \- \[%{MONTHDAY}\/%{MONTH}\/%{YEAR}\:%{HOUR}:?%{MINUTE}\:%{SECOND} %{ISO8601_TIMEZONE}\] \"%{WORD:method} %{GREEDYDATA:syslog_message}" }
    }
  } else if ("{{ COMMON_LOG_DIR }}/tracking/tracking.log" in [source]) {
    grok {
      match => { "message" => "%{GREEDYDATA:syslog_message}" }
    #{"username": "", "event_type": "/", "ip": "192.168.33.1", "agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/601.6.17 (KHTML, like Gecko) Version/9.1.1 Safari/601.6.17", "host": "precise64", "referer": "", "accept_language": "en-us", "event": "{\"POST\": {}, \"GET\": {}}", "event_source": "server", "context": {"user_id": "", "org_id": "", "course_id": "", "path": "/"}, "time": "2016-07-15T21:08:52.465455+00:00", "page": null}
    }
  } else if ("{{ COMMON_LOG_DIR }}/mongo/mongodb.log" in [source]) {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:syslog_timestamp} \[%{DATA:syslog_program}\] %{GREEDYDATA:syslog_message}" }
    }
  } else if ("{{ COMMON_LOG_DIR }}/rabbitmq/edx.cms.core.default.log" in [source]) {
    grok {
      match => { "message" => "%{GREEDYDATA:syslog_message}" }
      #date='Fri Jul 15 22:46:02 UTC 2016' vhost='/' queue='edx.cms.core.default' length=3
    }
  } else if ("{{ logstash_log_dir }}/logstash.log" in [source]) {
    grok {
      match => { "message" => "\{\:%{WORD}\=\>\"%{TIMESTAMP_ISO8601:syslog_timestamp}\"\, %{GREEDYDATA:syslog_message}\}" }
    }
  } else if ("{{ COMMON_LOG_DIR }}/lms/edx.log" in [source] or "{{ COMMON_LOG_DIR }}/cms/edx.log" in [source] or "{{ COMMON_LOG_DIR }}/motifier/edx.log" in [source]) {
    grok {
      match => { "message" => "%{SYSLOGTIMESTAMP:syslog} %{WORD:syslog_hostname} \[%{DATA:environment}\]\[%{DATA:syslog_program}\]\[%{DATA}] %{WORD:method} \[%{WORD}  %{POSINT:syslog_pid}\] \[%{DATA}\] \- %{GREEDYDATA:syslog_message}" }
      match => { "message" => "%{GREEDYDATA:syslog_message}" }
    }
  } else if ("{{ COMMON_LOG_DIR }}/devpi/supervisor/supervisord.log" in [source] or "{{ COMMON_LOG_DIR }}/supervisor/supervisord.log" in [source]) {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:syslog_timestamp} %{WORD:method} %{GREEDYDATA:syslog_message}" }
      match => { "message" => "%{GREEDYDATA:syslog_message}" }
    }
  } else if ("{{ elasticsearch_log_dir }}/elasticsearch.log" in [source]) {
    grok {
      match => { "message" => "(?:\[%{TIMESTAMP_ISO8601:syslog_timestamp}\]\[%{WORD:syslog_severity}\])%{GREEDYDATA:syslog_message}" }
    }
  } else { #matches all rsyslog messages
    grok {
      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{WORD:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
    }
  }
  syslog_pri { }
  date {
    match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss", "DD MMM YYYY:HH:mm:ss ZZZ", "ISO8601"]
  }
  # Try and parse the tracking log json
  # 142 is syslog facility 17 (local1) and Informational.
  # This is used to reduce the number of errors in json parsing as
  # tracking uses that facility and priority by default.
  if "142" in [syslog_pri] {
    json {
      source => "syslog_message"
      target => "tracking"
    }
  }
  if ([message] =~ /(?i)warn/) {
    mutate {
      add_tag => "warning"
    }
  }
  if ([message] =~ /(?i)error/) {
    mutate {
      add_tag => " error"
    }
  }
  if !("_grokparsefailure" in [tags]) {
    mutate {
      replace => [ "@source_host", "%{host}" ]
      replace => [ "@message", "%{syslog_message}" ]
    }
    mutate {
      remove_field => [ "syslog_hostname", "syslog_message", "syslog_timestamp" ]
      remove_tag => [ "beats_input_codec_plain_applied" ]
    }
  }
}

output {
  # Example just to output to elasticsearch
  elasticsearch { }
  # And gzip for each host and program
  file {
       path => '{{ logstash_data_dir }}/%{@source_host}/all.%{+yyyyMMdd}.gz'
       gzip => true
  }
  # Should add option for S3 as well.
}
